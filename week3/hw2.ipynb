{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import urllib\n",
    "import scipy.optimize\n",
    "import random\n",
    "from sklearn.decomposition import PCA\n",
    "from collections import defaultdict\n",
    "\n",
    "### PCA on beer reviews ###\n",
    "\n",
    "def parseData(fname):\n",
    "  for l in urllib.urlopen(fname):\n",
    "    yield eval(l)\n",
    "\n",
    "print \"Reading data...\"\n",
    "data = list(parseData(\"http://jmcauley.ucsd.edu/cse190/data/beer/beer_50000.json\"))\n",
    "print \"done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import scipy.optimize\n",
    "import random\n",
    "from math import exp\n",
    "from math import log\n",
    "\n",
    "def feature(datum):\n",
    "  feat = [1, datum['review/taste'], datum['review/appearance'], \\\n",
    "          datum['review/aroma'], datum['review/palate'], \\\n",
    "          datum['review/overall']]\n",
    "  return feat\n",
    "\n",
    "# first shuffle the data\n",
    "rand_data = numpy.copy(data)\n",
    "numpy.random.shuffle(rand_data)\n",
    "X = [feature(d) for d in rand_data]\n",
    "y = [d['beer/ABV'] >= 6.5 for d in rand_data]\n",
    "\n",
    "def inner(x,y):\n",
    "  return sum([x[i]*y[i] for i in range(len(x))])\n",
    "\n",
    "def sigmoid(x):\n",
    "  return 1.0 / (1 + exp(-x))\n",
    "\n",
    "##################################################\n",
    "# Logistic regression by gradient ascent         #\n",
    "##################################################\n",
    "\n",
    "# NEGATIVE Log-likelihood\n",
    "def f(theta, X, y, lam):\n",
    "  loglikelihood = 0\n",
    "  for i in range(len(X)):\n",
    "    logit = inner(X[i], theta)\n",
    "    loglikelihood -= log(1 + exp(-logit))\n",
    "    if not y[i]:\n",
    "      loglikelihood -= logit\n",
    "  for k in range(len(theta)):\n",
    "    loglikelihood -= lam * theta[k]*theta[k]\n",
    "  # for debugging\n",
    "  # print(\"ll =\" + str(loglikelihood))\n",
    "  return -loglikelihood\n",
    "\n",
    "# NEGATIVE Derivative of log-likelihood\n",
    "def fprime(theta, X, y, lam):\n",
    "  dl = [0]*len(theta)\n",
    "  for i in range(len(X)):\n",
    "    logit = inner(X[i], theta)\n",
    "    for k in range(len(theta)):\n",
    "      dl[k] += X[i][k] * (1 - sigmoid(logit))\n",
    "      if not y[i]:\n",
    "        dl[k] -= X[i][k]\n",
    "  for k in range(len(theta)):\n",
    "    dl[k] -= lam*2*theta[k]\n",
    "  return numpy.array([-x for x in dl])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda = 1.0:\taccuracy_train =\t0.7205088203528142\n",
      "lambda = 1.0:\taccuracy_tests =\t0.7143628509719222\n",
      "lambda = 1.0:\taccuracy_validation =\t0.7147485899435977\n"
     ]
    }
   ],
   "source": [
    "# 1. The code currently does not perform any train/test splits. \n",
    "# Split the data into training, validation, and test sets, \n",
    "# via 1/3, 1/3, 1/3 splits. Use random splits of the data \n",
    "# (i.e., each should be a random, non- overlapping sample of the data;\n",
    "# this can be obtained by first shuffling the data). After training on\n",
    "# the training set, report the accuracy of the classifier on the \n",
    "# validation and test sets (1 mark).\n",
    "\n",
    "\n",
    "# Split the data into training, validation, and test sets, \n",
    "# via 1/3, 1/3, 1/3 splits\n",
    "length = int(len(rand_data)/3)\n",
    "\n",
    "X_train = X[:length]\n",
    "y_train = y[:length]\n",
    "\n",
    "X_validation = X[length:2*length]\n",
    "y_validation = y[length:2*length]\n",
    "\n",
    "X_test = X[2*length:]\n",
    "y_test = y[2*length:]\n",
    "\n",
    "##################################################\n",
    "# Train                                          #\n",
    "##################################################\n",
    "\n",
    "def train(lam):\n",
    "  theta,_,_ = scipy.optimize.fmin_l_bfgs_b(f, [0]*len(X[0]), fprime, pgtol = 10, args = (X_train, y_train, lam))\n",
    "  return theta\n",
    "\n",
    "# train on training set\n",
    "lam = 1.0\n",
    "theta = train(lam)\n",
    "\n",
    "##################################################\n",
    "# Predict                                        #\n",
    "##################################################\n",
    "\n",
    "def performance(theta, set_x, set_y):\n",
    "  scores = [inner(theta,x) for x in set_x]\n",
    "  predictions = [s > 0 for s in scores]\n",
    "  correct = [(a==b) for (a,b) in zip(predictions,set_y)]\n",
    "  acc = sum(correct) * 1.0 / len(correct)\n",
    "  return acc\n",
    "\n",
    "# accuracy\n",
    "acc_train = performance(theta, X_train, y_train)\n",
    "print(\"lambda = \" + str(lam) + \":\\taccuracy_train =\\t\" + str(acc_train))\n",
    "\n",
    "acc_test = performance(theta, X_test, y_test)\n",
    "print(\"lambda = \" + str(lam) + \":\\taccuracy_tests =\\t\" + str(acc_test))\n",
    "\n",
    "acc_validate = performance(theta, X_validation, y_validation)\n",
    "print(\"lambda = \" + str(lam) + \":\\taccuracy_validation =\\t\" + str(acc_validate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positives\tNegatives\tTrue Positives\tTrue Negatives\tFalse Positives\tFalse Negatives\n",
      "10341.0\t\t6327.0\t\t9111.0\t\t2796.0\t\t3531.0\t\t1230.0\n"
     ]
    }
   ],
   "source": [
    "# 2. Report the number of Positives, Negatives, True Positives, \n",
    "# True Negatives, False Positives, and False Negatives using \n",
    "# the test set of the classifier you trained above (1 mark).\n",
    "def P(y_data):\n",
    "    posit = [(a==1) for a in y_data]\n",
    "    p = sum(posit) * 1.0\n",
    "    return p\n",
    "\n",
    "def N(y_data):\n",
    "    negat = [(a==0) for a in y_data]\n",
    "    n = sum(negat) * 1.0\n",
    "    return n\n",
    "    \n",
    "def TP(theta, X_data, y_data):\n",
    "    scores = [inner(theta,x) for x in X_data]\n",
    "    predictions = [s > 0 for s in scores]\n",
    "    correct = [((a==1) and (b==1)) for (a,b) in zip(predictions,y_data)]\n",
    "    tp = sum(correct) * 1.0\n",
    "    return tp\n",
    "def TN(theta, X_data, y_data):\n",
    "    scores = [inner(theta,x) for x in X_data]\n",
    "    predictions = [s > 0 for s in scores]\n",
    "    correct = [((a==0) and (b==0)) for (a,b) in zip(predictions,y_data)]\n",
    "    tn = sum(correct) * 1.0\n",
    "    return tn\n",
    "def FP(theta, X_data, y_data):\n",
    "    scores = [inner(theta,x) for x in X_data]\n",
    "    predictions = [s > 0 for s in scores]\n",
    "    correct = [((a==1) and (b==0)) for (a,b) in zip(predictions,y_data)]\n",
    "    fp = sum(correct) * 1.0\n",
    "    return fp\n",
    "def FN(theta, X_data, y_data):\n",
    "    scores = [inner(theta,x) for x in X_data]\n",
    "    predictions = [s > 0 for s in scores]\n",
    "    correct = [((a==0) and (b==1)) for (a,b) in zip(predictions,y_data)]\n",
    "    fn = sum(correct) * 1.0\n",
    "    return fn\n",
    "\n",
    "# Positeves\n",
    "p = P(y_test)\n",
    "# Negatives\n",
    "n = N(y_test)\n",
    "# True Positives\n",
    "tp = TP(theta, X_test, y_test)\n",
    "# True Negatives\n",
    "tn = TN(theta, X_test, y_test)\n",
    "# False Positives\n",
    "fp = FP(theta, X_test, y_test)\n",
    "# False Negatives\n",
    "fn = FN(theta, X_test, y_test)\n",
    "print (\"Positives\\tNegatives\\tTrue Positives\\tTrue Negatives\\tFalse Positives\\tFalse Negatives\")\n",
    "print (str(p)+'\\t\\t'+str(n)+'\\t\\t'+str(tp)+'\\t\\t'+str(tn)+'\\t\\t'+str(fp)+'\\t\\t'+str(fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. (Hard)Describehowyouwouldmodifythecodestubprovidedifyouwantedtoassigngreaterimportance\n",
    "# to False Positives compared to False Negatives. Suggest specific modifications \n",
    "# that would make to the code if you wanted to assign 10 times as much importance \n",
    "# to False Positives as compared to False Negatives.\n",
    "\n",
    "# Answer\n",
    "# So the basic idea is that: the loss function can be the contributed both by FP and FN,\n",
    "# and loss function also equals to −loglikelihood. so we should modify the function\n",
    "# f(theta, X, y, lam) which represents −loglikelihood.\n",
    "# the log likelihood equation：∑i−log(1+e−Xi.θ)+∑y=0−Xi.θ−λ||θ||2\n",
    "# FP means modle prediction is positive but it is negative in lable.\n",
    "# if we want to assign more importance on FP, we should multiple a factor\n",
    "# to the second part of the equation above\n",
    "def f(theta, X, y, lam):\n",
    "  loglikelihood = 0\n",
    "  for i in range(len(X)):\n",
    "    logit = inner(X[i], theta)\n",
    "    loglikelihood -= log(1 + exp(-logit))\n",
    "    ##########################################\n",
    "    if not y[i]:                             #\n",
    "        if logit > 0:                        # \n",
    "            loglikelihood -= logit * factor  # Say: factor can be 10.\n",
    "        else:                                #\n",
    "            loglikelihood -= logit           #\n",
    "    ##########################################\n",
    "  for k in range(len(theta)):\n",
    "    loglikelihood -= lam * theta[k]*theta[k]\n",
    "  # for debugging\n",
    "  # print(\"ll =\" + str(loglikelihood))\n",
    "  return -loglikelihood\n",
    "\n",
    "# NEGATIVE Derivative of log-likelihood\n",
    "def fprime(theta, X, y, lam):\n",
    "  dl = [0]*len(theta)\n",
    "  for i in range(len(X)):\n",
    "    logit = inner(X[i], theta)\n",
    "    for k in range(len(theta)):\n",
    "      dl[k] += X[i][k] * (1 - sigmoid(logit))\n",
    "    ##########################################\n",
    "      if not y[i]:                           #\n",
    "            if logit > 0:                    #\n",
    "                dl[k] -= X[i][k]*factor      # we add the factor here accordingly\n",
    "            else:                            #\n",
    "                dl[k] -= X[i][k]             #\n",
    "    ##########################################    \n",
    "  for k in range(len(theta)):\n",
    "    dl[k] -= lam*2*theta[k]\n",
    "  return numpy.array([-x for x in dl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t0.7158286331453259\n",
      "0.01\t0.7172086883475339\n",
      "0.1\t0.7155886235449418\n",
      "1\t0.7147485899435977\n",
      "100\t0.6665066602664107\n"
     ]
    }
   ],
   "source": [
    "# 4. Implement a training/validation/test pipeline so that you can select\n",
    "# the best model based on its perfor- mance on the validation set. \n",
    "# Try models with λ ∈ {0, 0.01, 0.1, 1, 100}. Report the performance on the\n",
    "# training/validation/test sets for the best value of λ (1 mark).\n",
    "\n",
    "LAM = [0, 0.01, 0.1, 1, 100]\n",
    "for lam in LAM:\n",
    "    theta = train(lam)\n",
    "    print str(lam) + '\\t' + str(performance(theta, X_validation, y_validation))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training\t\t0.7203288131525261\n",
      "testing   \t\t0.7170626349892009\n",
      "validation\t\t0.7172086883475339\n"
     ]
    }
   ],
   "source": [
    "# the best value of λ is 0.01\n",
    "lam = 0.01\n",
    "theta = train(lam)\n",
    "print \"training\\t\" + '\\t' + str(performance(theta, X_train, y_train))\n",
    "print \"testing   \\t\" + '\\t' + str(performance(theta, X_test, y_test))\n",
    "print \"validation\\t\" + '\\t' + str(performance(theta, X_validation, y_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 3 components in the graph\n",
      "there are 40 nodes in the largest components\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 5. How many connected components are in the graph, and how many \n",
    "# nodes are in the largest connected component (1 mark)?\n",
    "### Network visualization ###\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "edges = set()\n",
    "nodes = set()\n",
    "for edge in urllib.urlopen(\"http://jmcauley.ucsd.edu/cse255/data/facebook/egonet.txt\", 'r'):\n",
    "    x,y = edge.split()\n",
    "    x,y = int(x),int(y)\n",
    "    edges.add((x,y))\n",
    "    #edges.add((y,x))\n",
    "    nodes.add(x)\n",
    "    nodes.add(y)\n",
    "\n",
    "#print len(edges)\n",
    "G = nx.Graph()\n",
    "for e in edges:\n",
    "  G.add_edge(e[0],e[1])\n",
    "#print len(G.edges())\n",
    "nx.draw(G)\n",
    "plt.show()\n",
    "plt.clf()\n",
    "\n",
    "components = sorted(nx.connected_components(G), key = len, reverse=True)\n",
    "print \"there are \" + str(len(components)) + \" components in the graph\"\n",
    "print \"there are \" + str(len(components[0])) + \" nodes in the largest components\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4224058769513316"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6. What is the normalized-cut cost of the 50/50 split you found above (1 mark)?\n",
    "sorted_comp = sorted(components[0])\n",
    "c1 = sorted_comp[:len(sorted_comp)/2]\n",
    "c2 = sorted_comp[len(sorted_comp)/2:]\n",
    "\n",
    "def norm_cut(c1, c2, G):\n",
    "    s1 = sum([G.degree(v) for v in c1]) * 1.0\n",
    "    s2 = sum([G.degree(v) for v in c2]) * 1.0\n",
    "    return nx.cut_size(G, c1, c2) * (1/s1 + 1/s2) / 2.0\n",
    "     \n",
    "nc_0 = norm_cut(c1, c2, G)\n",
    "nc_0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum cost is 0.0981704596162\t\n",
      "cluster A: [825, 861, 863, 864, 876, 878, 882, 884, 886, 888, 889, 893, 804, 729]\n",
      "cluster B: [697, 703, 708, 713, 719, 745, 747, 753, 769, 772, 774, 798, 800, 803, 805, 810, 811, 819, 890, 880, 869, 840, 830, 828, 823, 856]\n"
     ]
    }
   ],
   "source": [
    "# 7. What are the elements of the split, and what is its normalized cut cost (1 mark)?\n",
    "# \n",
    "A = numpy.copy(c1).tolist()\n",
    "B = numpy.copy(c2).tolist()\n",
    "res = nc_0\n",
    "cur_min = (res,A,B)\n",
    "\n",
    "# function to find the best partition in each step\n",
    "def find_min(A,B,cur_min):\n",
    "    res = (0,[],[])\n",
    "    for i in range(len(A)):\n",
    "        B.append(A[i])\n",
    "        A.remove(A[i])\n",
    "        if norm_cut(A,B,G) < cur_min[0]:\n",
    "            a = numpy.copy(A).tolist()\n",
    "            b = numpy.copy(B).tolist()\n",
    "            res = (norm_cut(A,B,G), a, b)\n",
    "        tmp = B[len(B)-1]\n",
    "        B.remove(tmp)\n",
    "        A.insert(i,tmp)\n",
    "    return res\n",
    "\n",
    "\n",
    "for i in range(200):\n",
    "    ab = find_min(cur_min[1],cur_min[2],cur_min)\n",
    "    ba = find_min(cur_min[2],cur_min[1],cur_min)\n",
    "    if ab[0] == 0 and ba[0] == 0:\n",
    "        #print \"end at \" + str(i) + \" iteration\"\n",
    "        break\n",
    "    if ab[0] == 0:\n",
    "        cur_min = ba\n",
    "    elif ba[0] == 0:\n",
    "        cur_min = ab\n",
    "    elif ab[0] < ba[0]:\n",
    "        cur_min = ab\n",
    "    else:\n",
    "        cur_min = ba\n",
    "\n",
    "print \"The minimum cost is \" + str(cur_min[0]) + \"\\t\"\n",
    "print \"cluster A: \" + str(cur_min[1])\n",
    "print \"cluster B: \" + str(cur_min[2])\n",
    "#print nx.cut_size(G, cur_min[1], cur_min[2])       \n",
    "#print norm_cut(cur_min[1], cur_min[2],G)\n",
    "#print "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end at 11 iteration\n",
      "The maximum Network modularity is 0.338016528926\t\n",
      "cluster A: [697, 703, 708, 713, 719, 745, 747, 772, 774, 800, 803, 805, 810, 819, 823, 828, 830, 840, 880]\n",
      "cluster B: [729, 753, 769, 798, 804, 811, 825, 856, 861, 863, 864, 869, 876, 878, 882, 884, 886, 888, 889, 890, 893]\n"
     ]
    }
   ],
   "source": [
    "# 8. Re-implement your greedy algorithm above so that it maximizes \n",
    "# the modularity, rather than the normal- ized cut cost. Report \n",
    "# modularity values for the 50/50 split you find (1 mark).\n",
    "\n",
    "\n",
    "# get the Network modularity\n",
    "def max_modul(c1, c2, G):    \n",
    "    n1 = 0\n",
    "    n2 = 0\n",
    "    a1 = 0\n",
    "    a2 = 0\n",
    "    for edge in G.edges():\n",
    "        if (edge[0] in c1) and (edge[1] in c1):\n",
    "            n1 = n1 + 1\n",
    "            a1 = a1 + 2\n",
    "        elif (edge[0] in c2) and (edge[1] in c2):\n",
    "            n2 = n2 + 1\n",
    "            a2 = a2 + 2\n",
    "        else: \n",
    "            a1 += 1\n",
    "            a2 += 1   \n",
    "       \n",
    "    #print n1\n",
    "    e1 = n1*1.0 / len(G.edges())\n",
    "    e2 = n2*1.0 / len(G.edges())\n",
    "    a1 = a1*1.0 / (2 * len(G.edges()))\n",
    "    a2 = a2*1.0 / (2 * len(G.edges()))\n",
    "    q = (e1-a1*a1) + (e2-a2*a2)\n",
    "    return q\n",
    "\n",
    "# function to find the best partition in each step\n",
    "def find_max(A,B,G,cur_max):\n",
    "    res = (0,[],[], 1000)\n",
    "    for i in range(len(A)):\n",
    "        B.append(A[i])\n",
    "        A.remove(A[i])\n",
    "        modul = max_modul(A,B,G)\n",
    "        if modul > cur_max[0] or (modul == cur_max[0] and B[len(B)-1] < cur_max[3]):\n",
    "            a = numpy.copy(A).tolist()\n",
    "            b = numpy.copy(B).tolist()\n",
    "            res = (modul, a, b, B[len(B)-1])\n",
    "            cur_max = (modul, a, b, B[len(B)-1])\n",
    "        tmp = B[len(B)-1]\n",
    "        B.remove(tmp)\n",
    "        A.insert(i,tmp)\n",
    "    return res\n",
    "\n",
    "\n",
    "g = G.subgraph(components[0])\n",
    "\n",
    "C = numpy.copy(c1).tolist()\n",
    "D = numpy.copy(c2).tolist()\n",
    "\n",
    "res = max_modul(C,D,g)\n",
    "\n",
    "# last parameter for check the node id when there is a equal result\n",
    "# appears for moving two different nodes\n",
    "cur_max = (0,C,D,1000) \n",
    "\n",
    "# interate 200 times to make sure that we will get a point that can get\n",
    "# maximum result\n",
    "for i in range(200):  \n",
    "    ba = find_max(cur_max[2],cur_max[1],g,cur_max)\n",
    "    ab = find_max(cur_max[1],cur_max[2],g,cur_max)\n",
    "    if ab[0] == 0 and ba[0] == 0:\n",
    "        print \"end at \" + str(i) + \" iteration\"\n",
    "        break\n",
    "    if ab[0] == 0:\n",
    "        cur_max = ba\n",
    "    elif ba[0] == 0:\n",
    "        cur_max = ab\n",
    "    elif ab[0] > ba[0]:\n",
    "        cur_max = ab\n",
    "    else:\n",
    "        cur_max = ba\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "print \"The maximum Network modularity is \" + str(cur_max[0]) + \"\\t\"\n",
    "print \"cluster A: \" + str(sorted((cur_max[1])))\n",
    "print \"cluster B: \" + str(sorted((cur_max[2])))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
